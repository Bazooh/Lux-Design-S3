env_args:
  transform_obs: HybridTransformObs
  transform_action: SimplerActionWithSap
  memory: RelicPointMemory
  reward_smoothing: False
  reward_phases:
    0:
      type: dense
      weights:
        points_gained: 0.2
        points_discovered: 3.0
        relics_discovered: 3.0
        cells_discovered: 0.01
        energy_gained: 0.0001
        sap_tried: 0.01
        units_moved: 0.0001
        distance_to_spawn: 0.0001
    1:
      type: sparse-delta
      weights:
        points_gained: 1.0
    2:
      type: sparse
      weights:
        wins: 1.0

network:
  model: Pix2Pix_AC
  load_from_checkpoint: checkpoints/3phase_2025_02_21
  restore_to_cpu: False
  n_resblocks: 5
  n_channels: 128
  embedding_time: 8
  normalize_logits: True
  normalize_value: True
  action_masking: False
ppo:
  # Learning Args
  start_lr: 1e-4
  end_lr: 1e-5
  num_envs: 64
  num_steps: 32
  total_timesteps: 1e8
  update_epochs: 4
  num_minibatches: 32
  gamma: 1.0
  gamma_smoothing: False
  gae_lambda: 0.95
  clip_grad_norm: 0.5
  clip_eps: 0.2
  points_pred_coef: 0.1
  ent_coef: 0.025
  vf_coef: 0.5
  max_grad_norm: 0.5
  anneal_lr: True
  action_temperature: 1.0
  match_count_per_episode: 5
  seed: 0
  selfplay_freq_update: 1000
  play_against_latest_model_ratio: 0.25
  play_against_arena_jax_ratio: 0.0

  # Log Args
  verbose: 1
  use_wandb: wandb
  run_name: 3phase_ppo

  # Save Args
  save_checkpoint_path: 3phase
  save_checkpoint_freq: 1000

arena_jax:
  agent: NaiveAgent_Jax
  num_matches: 50
  arena_freq: 500
  record_freq: 500
# arena_std:
#   agent: RelicBoundAgent
#   num_matches: 10
#   arena_freq: 500
#   record_freq: 100