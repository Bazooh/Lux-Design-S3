env_args:
  reward_weights:
    0:
      stat_0: points_gained
      weight_0: 0.1
      stat_1: points_discovered
      weight_1: 2.0
      stat_2: relics_discovered
      weight_2: 2.0
      stat_3: cells_discovered
      weight_3: 0.01
    1:
      stat_0: points_gained
      weight_0: 0.1
      stat_1: points_discovered
      weight_1: 2.0
      stat_2: relics_discovered
      weight_2: 2.0
      stat_3: cells_discovered
      weight_3: 0.01
    2:
      stat_0: points_gained
      weight_0: 0.1
      stat_1: points_discovered
      weight_1: 2.0
      stat_2: relics_discovered
      weight_2: 2.0
      stat_3: cells_discovered
      weight_3: 0.01
  transform_obs: HybridTransformObs
  transform_action: SimplerActionWithSap
  memory: RelicPointMemory

network:
  model: Pix2Pix_AC
  load_from_checkpoint: None
  n_resblocks: 4
  n_channels: 128
  embedding_time: 6
  normalize_logits: True
  normalize_value: True

ppo:
  # Learning Args
  lr: 1e-4
  num_envs: 32
  num_steps: 128
  total_timesteps: 3e7
  update_epochs: 4
  num_minibatches: 16
  gamma: 0.99
  gae_lambda: 0.95
  clip_grad_norm: 0.5
  clip_eps: 0.2
  ent_coef: 0.1
  vf_coef: 0.5
  max_grad_norm: 0.5
  anneal_lr: True
  action_temperature: 1.0
  match_count_per_episode: 5
  seed: 0

  # Log Args
  use_wandb: True
  arena_freq: 50
  record_freq: 100
  arena_agent: NaiveAgent_Jax
  match_count_per_episode_arena: 5

  # Save Args
  save_checkpoint_path: save_exploration